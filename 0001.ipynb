{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca50e4b3",
   "metadata": {},
   "source": [
    "# Пример использования для доразметки установленного локально (а потому без абонентской платы) ИИ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b14539",
   "metadata": {},
   "outputs": [],
   "source": [
    "Тайминги получены на ПК с ОЗУ 32ГБ + ГПУ RTX3060."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21d56a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://habr.com/ru/articles/775870/\n",
    "#https://habr.com/ru/articles/769124/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a232d5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0babddde",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3090c93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bc2af98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers accelerate peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2908e929",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Могут возникать ошибки из-за несовместимости версий\n",
    "#pip install peft==0.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "188b76c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dm/anaconda3/envs/p310implicit/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f00a3534ae2746619e16db21dc22281c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "/home/dm/anaconda3/envs/p310implicit/lib/python3.10/site-packages/accelerate/utils/modeling.py:1462: UserWarning: Current model requires 520101632 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.3 s, sys: 18.8 s, total: 30.1 s\n",
      "Wall time: 43.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "MODEL_NAME = \"IlyaGusev/saiga_mistral_7b_lora\"\n",
    "\n",
    "# Загружаем модель\n",
    "config = PeftConfig.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "\tconfig.base_model_name_or_path,\n",
    "\ttorch_dtype=torch.float16,\n",
    "\tdevice_map=\"auto\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "\tmodel,\n",
    "\tMODEL_NAME,\n",
    "\ttorch_dtype=torch.float16\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# Определяем токенайзер\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Функция для обработки запросов\n",
    "def generate(model, tokenizer, prompt, generation_config):\n",
    "    data = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    #print(data)\n",
    "    data = {k: v.to(model.device) for k, v in data.items()}\n",
    "    output_ids = model.generate(\n",
    "        **data,\n",
    "        generation_config=generation_config\n",
    "    )[0]\n",
    "    #print(output_ids)\n",
    "    output_ids = output_ids[len(data[\"input_ids\"][0]):]\n",
    "    #print(output_ids)\n",
    "    output = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    return output.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c96d064",
   "metadata": {},
   "source": [
    "# Пример галлюцинации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83290d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Панамские острова расположены в Тихоокеанском регионе, примерно в 100 километрах к юго-востоку от Панамы. Они являются частью архипелага Сан-Блас, который включает в себя более чем 400 островов и атоллов.\n",
      "CPU times: user 1min 4s, sys: 1.76 s, total: 1min 6s\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Формируем запрос\n",
    "#PROMT_TEMPLATE = '<s>system\\nТы — Сайга, русскоязычный автоматический ассистент. Ты разговариваешь с людьми и помогаешь им.</s><s>user\\n{inp}</s><s>bot\\n'\n",
    "PROMT_TEMPLATE = '<s>user\\n{inp}</s><s>bot\\n'\n",
    "inp = 'Где находятся панамские острова?'\n",
    "prompt = PROMT_TEMPLATE.format(inp=inp)\n",
    "\n",
    "# Отправляем запрос в llm\n",
    "output = generate(model, tokenizer, prompt, generation_config)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ff5c67",
   "metadata": {},
   "source": [
    "# Заполнение пустой категории"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7de5eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Молоко\n",
      "2. Сметана\n",
      "3. Кефир\n",
      "4. Йогурт\n",
      "5. Мороженое\n",
      "6. Масло\n",
      "7. Сиреневый сыр\n",
      "8. Капучино\n",
      "9. Латте\n",
      "10. Млечный коктейль\n",
      "CPU times: user 53.4 s, sys: 1.38 s, total: 54.8 s\n",
      "Wall time: 54.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Формируем запрос\n",
    "#PROMT_TEMPLATE = '<s>system\\nТы — Сайга, русскоязычный автоматический ассистент. Ты разговариваешь с людьми и помогаешь им.</s><s>user\\n{inp}</s><s>bot\\n'\n",
    "PROMT_TEMPLATE = '<s>user\\n{inp}</s><s>bot\\n'\n",
    "inp = 'Составь список товаров категории молочные продукты.'\n",
    "prompt = PROMT_TEMPLATE.format(inp=inp)\n",
    "\n",
    "# Отправляем запрос в llm\n",
    "output = generate(model, tokenizer, prompt, generation_config)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfaa7fd",
   "metadata": {},
   "source": [
    "# Проверка соответствия товара к категории"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7426c288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "да\n",
      "CPU times: user 1.52 s, sys: 57.2 ms, total: 1.58 s\n",
      "Wall time: 1.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Формируем запрос\n",
    "PROMT_TEMPLATE = '<s>user\\n{inp}</s><s>bot\\n'\n",
    "inp = 'Относится ли кефир к молочным продуктам? Варианты ответа - да или нет.'\n",
    "prompt = PROMT_TEMPLATE.format(inp=inp)\n",
    "\n",
    "# Отправляем запрос в llm\n",
    "output = generate(model, tokenizer, prompt, generation_config)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888fd23e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccca4a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6eff43c",
   "metadata": {},
   "source": [
    "# IlyaGusev/saiga_yandexgpt_8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef281cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install accelerate\n",
    "#!pip install -i https://pypi.org/simple/ bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a930f0ad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb4f7d427a7b4cb99e50a4ade8c24d68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"max_length\": 1024000,\n",
      "  \"max_new_tokens\": 2048,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.95\n",
      "}\n",
      "\n",
      "Почему трава зеленая?\n",
      "Трава зеленая благодаря наличию в ее клетках особого пигмента, называемого хлорофиллом. Этот пигмент играет ключевую роль в процессе фотосинтеза – превращении солнечного света в энергию для роста и развития растений. Хлорофилл поглощает свет преимущественно в синей и красной частях спектра (400–500 нм и 620–730 нм соответственно), но отражает зеленый свет (около 500–570 нм). Именно поэтому мы видим траву зеленой: наше зрение воспринимает отраженный зеленый свет как зеленый цвет.\n",
      "\n",
      "==============================\n",
      "\n",
      "Сочини длинный рассказ, обязательно упоминая следующие объекты. Дано: Таня, мяч\n",
      "В одном уютном городке на окраине России жила девочка по имени Таня. Её дом стоял среди старых лип и яблонь, а за окном всегда играли солнечные лучи, освещая комнату тёплым светом. В углу комнаты, рядом с книжным шкафом, лежал старый футбольный мяч – его подарила Тане бабушка на день рождения несколько лет назад. Этот мячик был особенным: он имел потрескавшийся кожаный чехол, который пах старым деревом и немного табаком после бабушкиных сигарет. На нём были изображены яркие полосы и звёзды, словно нарисованные детской рукой. Мяч стал верным спутником всех игр Тани, от футбола до баскетбола, когда она подбрасывала его вверх или ловила в прыжке.\n",
      "\n",
      "Каждое лето город оживал. Улицы наполнялись смехом детей, играющих в футбол во дворах. Шумные компании собирались вокруг футбольных ворот, установленных между двух деревьев. И здесь неизменно появлялась Таня со своим мячом. Она была не просто игроком; её движения казались лёгкими и грациозными, будто мяч танцевал вместе с ней. Друзья прозвали её \"танцующей футболисткой\", ведь каждый раз, когда она ударяла по мячу ногой, казалось, что это делает не столько сама нога, сколько всё тело, включая руки и плечи.\n",
      "\n",
      "Однажды утром, когда солнце уже встало над горизонтом, Таня проснулась раньше обычного. Ей снился странный сон о гигантском футбольном матче, где все игроки были животными, а она управляла ими через свой мяч. Проснувшись, девушка почувствовала непреодолимое желание выйти на улицу. Погода стояла прекрасная: лёгкий ветерок приносил аромат цветущих каштанов, а небо было чистым и синим, как глаза её мамы.\n",
      "\n",
      "Выйдя из дома, Таня увидела необычное зрелище. Футбольные ворота стояли посреди улицы, окружённые людьми разных возрастов. Но самое удивительное было то, что вместо обычных игроков к воротам приближались различные животные: кошки, собаки, даже пара уток! Все они направлялись к Тане, словно искали какого-то лидера. Девушка поняла, что это тот самый матч из сна.\n",
      "\n",
      "\"Мяч,\" — прошептала она вслух, поглаживая знакомый предмет. \"Ты должен помочь!\"\n",
      "\n",
      "С каждым ударом мяча по земле животные начинали двигаться более слаженно. Собаки образовывали полукруг обороны, кошки прыгали высоко вверх, создавая атмосферу непредсказуемости и хитрости. Даже утки плавали синхронно возле края поля, готовясь поддержать атакующий прорыв.\n",
      "\n",
      "Игра шла невероятно интересно. Каждый удар мяча вызывал волну эмоций среди зрителей. Взрослые хлопали в ладоши, дети кричали и поддерживали своих любимцев. Особенно впечатляющим стало то, как команда Танюшки смогла забить гол против команды кошек. Мяч взлетел высоко над воротами, прежде чем приземлиться точно в центр, вызвав бурный восторг толпы.\n",
      "\n",
      "После игры животные разошлись по домам, но Таня осталась на улице. Вокруг неё собрались друзья, которые стали свидетелями удивительного зрелища. Они обсуждали стратегию матча, восхищались мастерством девушки и удивлялись тому, насколько сильно может влиять один маленький мяч на жизнь целого города.\n",
      "\n",
      "Бабушка, которая тоже наблюдала за происходящим из окна своего домика, вышла к ним с чашкой чая для каждого. \n",
      "\n",
      "— Вижу, ты сегодня сделала настоящее чудо, внучка, — улыбнулась старушка, протягивая чашку Тане. — А знаешь ли ты, что этот мяч обладает своей собственной магией? Он учит нас дружбе, командной работе и вере в себя.\n",
      "\n",
      "Таня кивнула, чувствуя теплоту внутри. Магия мяча действительно заключалась не только в его способности летать и прыгать, но и в том ощущении единства и радости, которое он дарил всем вокруг. С тех пор каждый вечер перед сном Таня рассказывала бабушке истории об удивительных приключениях этого старого футбольного мяча, о новых друзьях и незабываемых моментах, которые происходили благодаря ему.\n",
      "\n",
      "И так продолжалось много лет, пока городок не превратился в место, куда приезжали люди со всего мира, чтобы увидеть неповторимую магию футбольного мяча и узнать секреты успеха маленькой девочки Тани. Ведь иногда самое важное волшебство происходит прямо на наших глазах, если мы готовы открыть своё сердце новым возможностям и верить в силу дружбы и командного духа.\n",
      "\n",
      "==============================\n",
      "\n",
      "CPU times: user 2min 34s, sys: 16.4 s, total: 2min 50s\n",
      "Wall time: 3min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "MODEL_NAME = \"IlyaGusev/saiga_yandexgpt_8b\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    #device_map=\"auto\"\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "print(generation_config)\n",
    "\n",
    "inputs = [\"Почему трава зеленая?\", \"Сочини длинный рассказ, обязательно упоминая следующие объекты. Дано: Таня, мяч\"]\n",
    "for query in inputs:\n",
    "    prompt = tokenizer.apply_chat_template([{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": query\n",
    "    }], tokenize=False, add_generation_prompt=True)\n",
    "    data = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    data = {k: v.to(model.device) for k, v in data.items()}\n",
    "    data.pop(\"token_type_ids\", None)\n",
    "    output_ids = model.generate(**data, generation_config=generation_config)[0]\n",
    "    output_ids = output_ids[len(data[\"input_ids\"][0]):]\n",
    "    output = tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n",
    "    print(query)\n",
    "    print(output)\n",
    "    print()\n",
    "    print(\"==============================\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "820b8498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Относится ли 'ряженка' к 'молочным продуктам', да или нет?\n",
      "Да, ряженка относится к молочным продуктам. Это кисломолочный продукт, получаемый путём сквашивания молока с использованием специальных заквасок, включающих болгарскую палочку и термофильный стрептококк.\n",
      "\n",
      "==============================\n",
      "\n",
      "Относится ли 'снежок' к 'молочным продуктам'? Варианты ответа - да или нет.\n",
      "Нет, \"снежок\" не относится к молочным продуктам. Это разговорное название для снежного шара (сферического устройства с механизмом вращения, внутри которого находится снег), а также может обозначать различные виды сладостей на основе снега или мороженого. Однако в контексте молочных продуктов он не используется.\n",
      "\n",
      "==============================\n",
      "\n",
      "Составь список товаров категории молочные продукты.\n",
      "1. Молоко (цельное, обезжиренное, ультрапастеризованное)\n",
      "2. Сливки (жирные и нежирные)\n",
      "3. Сметана (различной жирности)\n",
      "4. Йогурт (натуральный, фруктовый, ягодный, овощной, бифидо-йогурты)\n",
      "5. Кефир (традиционный, биокефир, кефир с добавками)\n",
      "6. Творог (классический, зерненый, обезжиренный, творожные сырки)\n",
      "7. Сыры (твердые, мягкие, плавленые, рассольные)\n",
      "8. Сырники (с различными наполнителями)\n",
      "9. Молочные коктейли\n",
      "10. Мороженое (ванильное, шоколадное, фруктовое, карамельное)\n",
      "11. Пудинг молочный\n",
      "12. Пахта (обезжиренные сливки после сепарирования молока)\n",
      "13. Кумыс (кисломолочный напиток из кобыльего молока)\n",
      "14. Мацони (грузинский йогурт)\n",
      "15. Рикотта (мягкий сыр итальянского происхождения)\n",
      "16. Фромаж (французский мягкий сыр)\n",
      "17. Простокваша (продукт естественной ферментации молока)\n",
      "18. Ацидофилин (кисломолочный продукт на основе ацидофильной палочки)\n",
      "19. Лактобациллин (пробиотик для поддержания микрофлоры кишечника)\n",
      "20. Сухое молоко (порошковый концентрат цельного или обезжиренного молока)\n",
      "21. Кисломолочное масло (масло, полученное путем взбивания сливок)\n",
      "22. Казеин (белок молока, используемый в производстве различных продуктов)\n",
      "\n",
      "==============================\n",
      "\n",
      "CPU times: user 1min 1s, sys: 28.8 ms, total: 1min 1s\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "inputs = [\"Относится ли 'ряженка' к 'молочным продуктам', да или нет?\", \n",
    "          \"Относится ли 'снежок' к 'молочным продуктам'? Варианты ответа - да или нет.\",\n",
    "          \"Составь список товаров категории молочные продукты.\"]\n",
    "for query in inputs:\n",
    "    prompt = tokenizer.apply_chat_template([{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": query\n",
    "    }], tokenize=False, add_generation_prompt=True)\n",
    "    data = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    data = {k: v.to(model.device) for k, v in data.items()}\n",
    "    data.pop(\"token_type_ids\", None)\n",
    "    output_ids = model.generate(**data, generation_config=generation_config)[0]\n",
    "    output_ids = output_ids[len(data[\"input_ids\"][0]):]\n",
    "    output = tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n",
    "    print(query)\n",
    "    print(output)\n",
    "    print()\n",
    "    print(\"==============================\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbb60da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
